<head>
<link rel="stylesheet" href="../style.css">
</head>

<h1>
Alignment Bug
</h1>
<h4>
September 26, 2019
</h4>

<hr>

<h3>
The Issue
</h3>

<p>
For the past few years, I've been using <code>jemalloc</code> to allocate memory
in my experiments, partly because of the usefulness of arena allocation. Because
these arenas allocate memory in multiple-megabyte chunks (or
<code>extents</code>, as they're called in <code>jemalloc</code>), this
allocation does not necessarily happen each time you request memory from the
arena. Instead, <code>jemalloc</code> allocates an extent only when needed, then
holds onto that extent until the arena is freed or until some other heuristic
determines that it should be freed.
</p>

<p>
However, for my research, I need to be able to bind all arena memory to a
specific NUMA node; that is, for every extent allocation, I need to call
<code>mbind</code> on that range of addresses. To do something like this,
<code>jemalloc</code> provides the <code>extent_hook</code> interface. Briefly,
this feature allows you to define a set of function pointers that will be used
for all memory operations in the arena: allocating, deallocating, committing,
etc. Notably, we define a function called <code>sa_alloc</code>, which allocates
a new extent to an arena.
</p>

<p>
<code>sa_alloc</code> accepts quite a few arguments, including the size of the extent that
it should allocate (<code>size</code>), and the number of bytes that it should align that
allocation by (<code>alignment</code>). The first argument is handled easily: simply call
<code>mmap</code> and pass it the correct size. The second argument is a little trickier,
but this is how the original code did it. I'll remove the error handling and
other distracting elements. This code was also not originally written by me, so
I've added my own comments:
</p>

<pre>
<code>
/* Do  the initial allocation */
ret = mmap(new_addr, size, PROT_READ | PROT_WRITE, mmflags, sa->fd, sa->size);

/* Check if the allocation meets the alignment */
if (alignment == 0 || ((uintptr_t) ret)%alignment == 0) {
  goto success;
}

/* If it's not aligned properly, unmap and try again with a larger size */
munmap(ret, size);
size += alignment;
ret = mmap(NULL, size, PROT_READ | PROT_WRITE, mmflags, sa->fd, sa->size);

/* Chop off and unmap the excess */
n = (uintptr_t) ret;
m = n + alignment - (n%alignment);
munmap(ret, m-n);
ret = (void *) m;

/* Finally, call 'mbind' on the new extent */
success:
  mbind(ret, size, mpol, nodemaskp, maxnode, MPOL_MF_MOVE) < 0);
</code>
</pre>

<p>
For several years, this worked for a variety of applications and never threw any
errors.  However, only recently and in certain situations, it began throwing
extremely rare errors in one particular application: AMG. Specifically, the
final <code>mbind</code> call would return <code>Bad address</code>, the
function would fail, and the runtime library would fail to allocate to an arena.
I should also note that while <code>perror</code> prints out <code>Bad
address</code>, it's possible that the error is caused by some of the other
arguments. Poking through the <code>do_mbind</code> function in the Linux
kernel, it seems as if there are a great deal of other things that could cause
that same <code>EFAULT</code> error.
</p>

<h3>
The Search
</h3>

<p>
To begin with, I didn't even consider the possibility that this code could be
flawed.  After all, it succeeded for several years before this, and I'd never
seen it throw an error in the past. Nearly every day, I would run experiments
that required this block of code, yet only now was it failing. After quickly
printing out the arguments to <code>mbind</code> (they looked entirely ordinary), I
started my search with what I considered to be more error-prone parts of the
codebase.
</p>

<p>
Being an error that happens every so often, my first though was a race
condition: something, I thought, must be interfering with this allocation in
some way, perhaps changing some of the heap-allocated structures from which I
get some of the arguments to <code>mbind</code>. However, all of these variables are
protected by an arena-wide mutex, so it's not possible for two threads to
allocate to a single arena simultaneously.
</p>

<p>
The last thing that I considered was that something had gone awry with onlining
some of the memory nodes. As in my previous post, I had some difficulty with
some of the blocks of memory on my system being <code>ZONE_MOVABLE</code>. This being a
recent problem, coupled with the fact that the error failed more consistently
when memory spilled onto other NUMA nodes, convinced me that <code>mbind</code> wasn't
able to allocate to certain regions of one of the NUMA nodes. That, however,
was quickly debunked by simply binding to different nodes, which resulted in
the same issue.
</p>

<h3>
The Solution
</h3>

<p>
Finally, I took a harder look at the alignment code: were there rare situations
in which it could fail, depending on some race condition? Printing out <code>m</code>,
<code>n</code>, <code>ret</code>, etc., I suddenly realized the issue: after failing to get the
correct alignment, the code adds <code>alignment</code> to the <code>size</code>. Then, after
adjusting the pointer and unmapping the excess, <code>size</code> remains the same. This
then gets passed to <code>mbind</code>, which is looking for a block of memory of size
<code>size</code>. However, because part of this allocated block was subsequently unmapped,
the size needs to be updated to reflect that.
</p>

<p>
Now, why did it take so long for this issue to finally crop up? How could I run
experiments for several years without experiencing this issue even once? I think that
in order for this issue to actually occur, a perfect storm of conditions must be true:
</p>

<ol>
    <li>
    The application must request a large amount of memory aligned by a size
    larger than the page size.
    </li>
    
    <li>
    The application must be extremely heavily threaded.
    </li>
    
    <li>
    The application must use lots of allocations and deallocations from those
    threads.
    </li>
</ol>
